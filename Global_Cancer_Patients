#Import needed libraries
import pandas as pd
dataset = pd.read_csv('Global_Cancer_Patients.csv')
dataset

# Display the number of rows and columns in the dataset ( rows × columns)
dataset.shape

#Display all the columns
dataset.columns

# Display summary statistics (count, mean, std, min, quartiles, max) for all numeric columns in the dataset
dataset.describe()

# dataset.head() shows the first five rows of the dataset.
dataset.head()

# Display the last 3 rows only
dataset.tail(3)

# Display number of Cancer_Type without any duplicate values
len(dataset['Cancer_Type'].unique())

# Delete the record which are blanks
dataset=dataset.dropna(how="all")
dataset

# Delete the record which are blanks
dataset=dataset.dropna(how="any")
dataset

sample = dataset.sample(frac=0.2, random_state=42)

# Step 1: Take 20% sample as the population
population = dataset.sample(frac=0.2, random_state=42)

# Step 2: Draw a random sample of size 150 from this population
sample_150 = population.sample(n=150, random_state=42)

# Step 3: Define descriptive statistics function
def descriptive_stats(column):
    stats = {
        "Mean": column.mean(),
        "Median": column.median(),
        "Mode": column.mode(),
        "Standard Deviation": column.std(),
        "Variance": column.var(),
        "Minimum": column.min(),
        "Maximum": column.max(),
        "Five-Number Summary": column.describe()[['min','25%','50%','75%','max']],
        "IQR": column.quantile(0.75) - column.quantile(0.25),
        "Skewness": column.skew(),
        "Kurtosis": column.kurt()
    }
    return stats

# Step 4: Apply function to Target_Severity_Score column
results = descriptive_stats(sample_150['Target_Severity_Score'])

# Display results
for key, value in results.items():
    print(f"{key}: \n{value}\n")


# Step 1: Take 20% sample as the population
population = dataset.sample(frac=0.2, random_state=42).reset_index(drop=True)

# Step 2: Define systematic sampling function
def systematic_sample(data, sample_size, start=0):
    n = len(data)
    k = n // sample_size   # step size
    indices = list(range(start, n, k))[:sample_size]
    return data.iloc[indices]

# Step 3: Draw systematic sample of size 150 
sample_sys = systematic_sample(population, 150)

# Step 4: Define descriptive statistics function
def descriptive_stats(column):
    stats = {
        "Mean": column.mean(),
        "Median": column.median(),
        "Mode": column.mode(),
        "Standard Deviation": column.std(),
        "Variance": column.var(),
        "Minimum": column.min(),
        "Maximum": column.max(),
        "Five-Number Summary": column.describe()[['min','25%','50%','75%','max']],
        "IQR": column.quantile(0.75) - column.quantile(0.25),
        "Skewness": column.skew(),
        "Kurtosis": column.kurt()
    }
    return stats

# Step 5: Apply function to dependent variable (Target_Severity_Score)
results = descriptive_stats(sample_sys['Target_Severity_Score'])

# Display results
for key, value in results.items():
    print(f"{key}: \n{value}\n")

# Hypothesis Testing
#### Assumption: 
Normally distributed

#### Purpose:<br>
To test whether patient age is significantly correlated with Target Severity Score among global cancer patients.

#### Alternative Hypothesis : 
There is a significant correlation between Age and Target Severity Score.<br>

If p ≥ 0.05 → variable is normally distributed<br>

If p < 0.05 → variable is not normally distributed<br>

#### Conclusion:
The p-value from both Spearman and Pearson tests is greater than the typical significance level of 0.05.<br>
This indicates that there is no statistically significant correlation between Age and Target Severity Score.<br>
Thus, the null hypothesis is accepted — Age does not significantly correlate with severity level in this sample.


# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt

# Extract AGE and Target Severity Score columns
ages = sample["Age"]
severity_scores = sample["Target_Severity_Score"]  # Replace with actual column name if different

# Create the scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(ages, severity_scores, color="#FF69B4", alpha=0.6, edgecolors="black")

# Add labels and title
plt.xlabel("Age of Patient")
plt.ylabel("Target Severity Score")
plt.title("Scatter Plot of Age vs Severity Score")

# Display the plot
plt.grid(True)
plt.tight_layout()
plt.show()


from scipy.stats import shapiro

# Test normality for AGE and Target Severity Score
age_stat, age_p = shapiro(sample["Age"])
severity_stat, severity_p = shapiro(sample["Target_Severity_Score"])

print(f"Shapiro-Wilk p-value for AGE: {age_p:.4e}")
print(f"Shapiro-Wilk p-value for Target Severity Score: {severity_p:.4e}")


### Spearman
from scipy.stats import spearmanr

r_spearman, p_spearman = spearmanr(sample["Age"], sample["Target_Severity_Score"])
print(f"Spearman's correlation coefficient: {r_spearman:.4f}")
print(f"P-value: {p_spearman:.4f}")

### Pearson

#Perform Pearson Correlation to check Age and Target_Severity_Score are correlated
# Perform Pearson's Correlation
from scipy.stats import pearsonr

r, p_value = pearsonr(sample['Age'], sample['Target_Severity_Score'])

# Print only the Pearson correlation coefficient
print(f"Pearson's Correlation Coefficient: {r :.4f}")
print(f"P_Value: {p_value :.4f}")

# Step 1: Identify all numeric columns in the dataset
numeric_columns = sample.select_dtypes(include=["number"]).columns

# Step 2: Compute Pearson correlation matrix
correlation_matrix = sample[numeric_columns].corr(method="pearson")

# Step 3: Display the correlation matrix
print("Pearson Correlation Matrix (Numeric Variables):")
print(correlation_matrix)


## Correlation of Categorical Variables – Chi-square Test

#### Purpose: 
To check whether Age group is independent of or dependent on Target Severity Score category.

#### Assumption: 
Independent observations; expected frequency in each cell of the contingency table should be ≥ 25.

#### How To: 
Perform correlation analysis using the Chi-square test.

#### Null Hypothesis (H₀): 
Age group is independent of Target Severity Score category (not related).

#### Alternative Hypothesis (H₁): 
Age group is dependent on Target Severity Score category (related).

# Categorize AGE into groups
sample['Age_Group'] = pd.cut(sample['Age'], bins=[0, 30, 50, 70, 100], labels=['<30', '30-50', '50-70', '70+'])

# Categorize Target Severity Score into levels
sample['Severity_Category'] = pd.cut(sample['Target_Severity_Score'], bins=[0, 3, 6, 9], labels=['Low', 'Moderate', 'High'])

# Create cross-tabulation between Age group and Target Severity Score category
contingency_data = pd.crosstab(sample['Age_Group'], sample['Severity_Category'], margins=False)
print(contingency_data)

contingency_data = pd.crosstab(sample['Age_Group'], sample['Severity_Category'], margins=False)

# Check Chi-square association between Age group and Severity category

from scipy.stats import chi2_contingency

# Perform Chi-square test on the contingency table
chi2_stat, p_value, dof, expected = chi2_contingency(contingency_data)

# Display results
print(f"Chi-square statistic: {chi2_stat:.3f}, p-value: {p_value:.3f}")

# Interpret the result
if p_value > 0.05:
    print("Age group and Severity category are independent.")
else:
    print("Age group and Severity category are dependent.")


## Parametric Test – One-Sample T-Test

#### Assumption: 
Data is normally distributed.

We aim to assess the severity level of global cancer patients. Specifically, we want to determine whether the sample’s average Target Severity Score is representative of a known reference population. For this one-sample t-test, we analyze the Target Severity Score as the single variable of interest.

#### Null Hypothesis (H₀):
The average Target Severity Score of global cancer patients is equal to the reference population mean.

#### Alternative Hypothesis (H₁): 
The average Target Severity Score of global cancer patients is not equal to the reference population mean.

# Create cross-tabulation between Cancer Stage and Target Severity Score category
# First, bin the Target Severity Score into categories
sample["Severity_Category"] = pd.cut(sample["Target_Severity_Score"], bins=[0, 3, 6, 9], labels=["Low", "Moderate", "High"])

# Then, build the contingency table
contingency_data = pd.crosstab(sample["Cancer_Stage"], sample["Severity_Category"], margins=False)
print(contingency_data)

#Check for normality
from scipy.stats import shapiro
r, p = shapiro(sample["Target_Severity_Score"])
print('r=%.2f, p=%.30f' % (r, p))
if p > 0.05:
    print('Normal distribution')
else:
    print('Not a normal distribution')


from scipy.stats import ttest_1samp

# Perform one-sample t-test comparing sample mean to reference value (e.g., 5.8)
t_stat, p_value = ttest_1samp(sample["Target_Severity_Score"], 5.8)

# Display results
print(f"T-statistic: {t_stat:.4f}, p-value: {p_value:.4f}")

# Interpretation
if p_value > 0.05:
    print("No significant difference from reference mean")
else:
    print("Significant difference from reference mean")


## Encoding Categorical Variable

from sklearn.preprocessing import LabelEncoder


# Initialize LabelEncoder
le = LabelEncoder()

# Fit and transform the 'Outcome' column
dataset['Cancer_Stage_encoded'] = le.fit_transform(dataset['Cancer_Stage'])

# Display the mapping and encoded values
print("Classes:", le.classes_)
print(dataset['Cancer_Stage_encoded'])

X = dataset.iloc[:, [5, 8]].values
y = dataset.iloc[:,14].values
print(X)
print(y)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/3, random_state=0)


from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Cluster

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Encode all categorical columns
label_encoder = LabelEncoder()
for column in dataset.columns:
    if dataset[column].dtype == 'object':
        dataset[column] = label_encoder.fit_transform(dataset[column])

# Select features for clustering (e.g., AGE and SMOKING)
X = dataset[['Genetic_Risk', 'Smoking']].values

# Apply the Elbow Method
wcss = []
for n_clusters in range(1, 11):
    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

# Plot the Elbow Curve
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--', color='blue')
plt.title('Elbow Method for Optimal Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.grid(True)
plt.tight_layout()


plt.savefig('Elbow Method for Optimal Clusters.png', dpi=300, bbox_inches='tight')
plt.show()


import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# --- 1) Prepare full data (for fitting) ---
full_X = dataset[['Genetic_Risk','Smoking','Obesity_Level']].values

# --- 2) Take a random sample for plotting only ---
SAMPLE_FRAC = 0.25          # 25% of rows; change to e.g. 0.1 for 10%
sampled_idx = dataset.sample(frac=SAMPLE_FRAC, random_state=42).index
X_plot = dataset.loc[sampled_idx, ['Genetic_Risk','Smoking','Obesity_Level']].values

# (If you prefer a fixed sample size, comment the two lines above and use:)
# SAMPLE_N = 3000
# sampled_idx = dataset.sample(n=min(SAMPLE_N, len(dataset)), random_state=42).index
# X_plot = dataset.loc[sampled_idx, ['Genetic_Risk','Smoking','Obesity_Level']].values

# --- 3) KMeans on FULL data (more stable clusters) ---
kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)
kmeans.fit(full_X)
centroids_plot = kmeans.cluster_centers_

# Predict cluster labels for the SAMPLED points (for plotting)
y_kmeans = kmeans.predict(X_plot)

# --- 4) 3D plot (same style/colors as your 2D plot) ---
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, projection='3d')

colors = ['red','blue','green','cyan']
labels = [
    'Low Genetic Risk, High Smoking',
    'Moderate Risk & Smoking',
    'Low Risk & Low Smoking',
    'High Genetic Risk, Low Smoking'
]

for i, col in enumerate(colors):
    ax.scatter(
        X_plot[y_kmeans == i, 0],
        X_plot[y_kmeans == i, 1],
        X_plot[y_kmeans == i, 2],
        s=18, c=col, marker='o', edgecolors='none', alpha=0.9, label=labels[i]
    )

# Centroids as big yellow dots (like your 2D plot)
ax.scatter(
    centroids_plot[:, 0], centroids_plot[:, 1], centroids_plot[:, 2],
    s=300, c='yellow', marker='o', edgecolors='black', linewidths=1.3,
    depthshade=False, zorder=100, label='Centroids'
)

# Keep everything visible
allX = np.vstack([X_plot, centroids_plot])
ax.set_xlim(allX[:,0].min(), allX[:,0].max())
ax.set_ylim(allX[:,1].min(), allX[:,1].max())
ax.set_zlim(allX[:,2].min(), allX[:,2].max())

ax.set_title('3D Clusters of Genetic Risk, Smoking, and Obesity Level (Random Sample)')
ax.set_xlabel('Genetic Risk')
ax.set_ylabel('Smoking')
ax.set_zlabel('Obesity Level')

ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1))
plt.tight_layout()

plt.savefig('3D Clusters of Genetic Risk, Smoking, and Obesity Level (Random Sample).png', dpi=300, bbox_inches='tight')
plt.show()


import pandas as pd
import matplotlib.pyplot as plt
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering

# 1) Load the dataset (change the path if needed)
df = pd.read_csv('Global_Cancer_Patients.csv')

# 2) Take a random sample to make dendrogram and clustering faster and clearer
df_sample = df.sample(n=500, random_state=42)

# 3) Select the features for clustering
X = df_sample[['Genetic_Risk', 'Smoking']].values

# 4) Plot the dendrogram
plt.figure(figsize=(10, 7))
sch.dendrogram(sch.linkage(X, method='ward'))
plt.title('Dendrogram for Global Cancer Patients (Sample of 500)')
plt.xlabel('Patients')
plt.ylabel('Euclidean distances')
plt.show()

# 5) Agglomerative Clustering (use metric instead of affinity in new sklearn)
hc = AgglomerativeClustering(n_clusters=5, metric='euclidean', linkage='ward')

# Fit the model and get cluster labels
y_hc = hc.fit_predict(X)

# 6) Plot the clusters
plt.figure(figsize=(8, 6))
for cluster_id in range(5):
    plt.scatter(
        X[y_hc == cluster_id, 0],
        X[y_hc == cluster_id, 1],
        s = 40,
        label = f'Cluster {cluster_id + 1}'
    )

plt.title('Clusters of Global Cancer Patients (Genetic_Risk vs Smoking)')
plt.xlabel('Genetic_Risk')
plt.ylabel('Smoking')
plt.legend()

plt.savefig('Clusters of Global Cancer Patients (Genetic_Risk vs Smoking).png', dpi=300, bbox_inches='tight')
plt.show()

## Linear Regression

from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)

# === Simple Linear Regression on Global Cancer Dataset (Sample-only, Red Line, with p-value) ===

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

from scipy.stats import linregress   # for p-value

# === Load dataset ===
# Use the same path style as your labs
df = pd.read_csv('Global_Cancer_Patients.csv')
# If your file is in the same folder as the notebook, use:
# df = pd.read_csv('Global_Cancer_Patients.csv')

# Choose columns
X_col = 'Genetic_Risk'
y_col = 'Target_Severity_Score'


def fit_and_plot_sample(dataframe, sample_frac=0.20, out_path="linreg_sample20_red.png"):
    """
    Take a random sample of the data (e.g., 20%), 
    fit a simple linear regression model (Genetic_Risk -> Target_Severity_Score),
    and plot the test points with a red regression line plus R^2, RMSE, and p-value.
    """

    # --- 1. Take sample only ---
    df_s = dataframe.sample(frac=sample_frac, random_state=7)

    # Keep only the two columns and drop missing values
    data = df_s[[X_col, y_col]].dropna()
    X = data[[X_col]].values
    y = data[y_col].values

    # --- 2. Train–test split ---
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # --- 3. Fit Linear Regression model ---
    model = LinearRegression()
    model.fit(X_train, y_train)

    # Predictions on test set
    y_pred = model.predict(X_test)

    # --- 4. Smooth red regression line for plotting ---
    x_line = np.linspace(X_test.min(), X_test.max(), 400).reshape(-1, 1)
    y_line = model.predict(x_line)

    # --- 5. Metrics (R² and RMSE) ---
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    coef = float(model.coef_[0])      # slope
    inter = float(model.intercept_)   # intercept

    # --- 6. p-value for the slope (using scipy.linregress on training data) ---
    x = X_train.ravel()
    y_tr = y_train.ravel()

    lin_res = linregress(x, y_tr)
    p_value = lin_res.pvalue

    # --- 7. Plot test points + red regression line ---
    plt.figure(figsize=(7, 5))
    plt.scatter(X_test, y_test, alpha=0.8, label='Test data')
    plt.plot(x_line, y_line, color='red', linewidth=3, label='Regression line')

    plt.title(f"{X_col} vs {y_col} (Sample {int(sample_frac*100)}%, Test set)")
    plt.xlabel(X_col.replace("_", " "))
    plt.ylabel(y_col.replace("_", " "))

    # Equation and metrics text
    eq = (
        f"{y_col} = {inter:.3f} + ({coef:.3f} × {X_col})\n"
        f"R² = {r2:.3f} | RMSE = {rmse:.3f} | p-value = {p_value:.4f}"
    )

    # Put text under the plot
    plt.gcf().text(0.02, -0.10, eq)
    plt.legend()
    plt.tight_layout()

    # Save and show
    plt.savefig(out_path, bbox_inches="tight")
    plt.show()

    print("Saved:", out_path)
    print(eq.replace("\n", " | "))


# === Run on 20% sample ===
fit_and_plot_sample(df, sample_frac=0.20, out_path="linreg_sample20_red.png")

plt.savefig('Linear_Regression.png', dpi=300, bbox_inches='tight')
plt.show()

# === Multiple Linear Regression on Global Cancer Dataset ===

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# === 1. Load dataset ===
# Use the path that matches your notebook location
# Example if the file is in the same folder:
df = pd.read_csv('Global_Cancer_Patients.csv')
# If it is in a "Data" folder, use:
# df = pd.read_csv('Data/Global_Cancer_Patients.csv')

# === 2. Select features (X) and target (y) ===
features = [
    'Age',
    'Genetic_Risk',
    'Air_Pollution',
    'Alcohol_Use',
    'Smoking',
    'Obesity_Level',
    'Treatment_Cost_USD',
    'Survival_Years'
]

target = 'Target_Severity_Score'

X = df[features].values
y = df[target].values

# === 3. Split into Train and Test sets ===
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# === 4. Scale the features (StandardScaler) ===
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# === 5. Train the Multiple Linear Regression model ===
reg = LinearRegression()
reg.fit(X_train, y_train)

# === 6. Predict on Test set ===
y_pred = reg.predict(X_test)

# === 7. Evaluate the model ===
accTrain = reg.score(X_train, y_train)              # R^2 on train
accTest = reg.score(X_test, y_test)                 # R^2 on test
error = mean_squared_error(y_test, y_pred)          # MSE on test

print(f"Accuracy Train (R^2): {accTrain:.8f}")
print(f"Accuracy Test  (R^2): {accTest:.8f}")
print(f"MSE: {error:.8f}")

# === 8. Visualization: Actual vs Predicted (same format as lab) ===
import numpy as np
import matplotlib.pyplot as plt

plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred, alpha=0.6)

# 45° reference line
minv = np.min([y_test.min(), y_pred.min()])
maxv = np.max([y_test.max(), y_pred.max()])
plt.plot([minv, maxv], [minv, maxv], linewidth=2)   # reference line

plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Multiple Linear Regression: Actual vs Predicted (Test)')
plt.tight_layout()

# Optional: save the figure
# plt.savefig('mlr_actual_vs_pred_test.png', bbox_inches='tight')

plt.savefig('Multiple Linear Regression: Actual vs Predicted (Test).png', dpi=300, bbox_inches='tight')
plt.show()


# === 8. Visualization: Actual vs Predicted (Test Set) ===
import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred, alpha=0.6)

# 45° reference line
minv = np.min([y_test.min(), y_pred.min()])
maxv = np.max([y_test.max(), y_pred.max()])
plt.plot([minv, maxv], [minv, maxv], linewidth=2, color='red')

plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Multiple Linear Regression: Actual vs Predicted (Test)')
plt.tight_layout()

# Save the figure to a file
plt.savefig('Actual_vs_Predicted_Regression.png', dpi=300, bbox_inches='tight')

# Display the plot
plt.show()

